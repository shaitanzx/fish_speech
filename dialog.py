import os
import json
os.environ["TORCH_COMPILE"] = "0"
os.environ["TORCHDYNAMO_DISABLE"] = "1"
root_dir = os.path.dirname(os.path.abspath(__file__))
outputs_dir = os.path.join(root_dir, "outputs")
os.makedirs(outputs_dir, exist_ok=True)
temp_dir = os.path.join(root_dir, "temp")
os.makedirs(temp_dir, exist_ok=True)

import shutil

for filename in os.listdir(temp_dir):
    file_path = os.path.join(temp_dir, filename)
    try:
        if os.path.isfile(file_path) or os.path.islink(file_path):
            os.unlink(file_path)
        elif os.path.isdir(file_path):
            shutil.rmtree(file_path)
    except Exception as e:
        print(f'Error delete in {file_path}. {e}')

os.environ["GRADIO_TEMP_DIR"] = temp_dir
import queue
from huggingface_hub import snapshot_download
import numpy as np
import wave
import io
import gc
from datetime import datetime
import html
import threading
from argparse import ArgumentParser
from pathlib import Path
from functools import partial

import gradio as gr
import librosa
import torch
import torchaudio

torchaudio.set_audio_backend("soundfile")

from loguru import logger
from transformers import AutoTokenizer

from fish_speech.i18n import i18n
from fish_speech.text.chn_text_norm.text import Text as ChnNormedText
from fish_speech.utils import autocast_exclude_mps, set_seed
from tools.api import decode_vq_tokens, encode_reference
from tools.file import AUDIO_EXTENSIONS, list_files
from tools.llama.generate import (
    GenerateRequest,
    GenerateResponse,
    WrappedGenerateResponse,
    launch_thread_safe_queue,
)
from tools.vqgan.inference import load_model as load_decoder_model

from tools.schema import (
    GLOBAL_NUM_SAMPLES,
    ServeTTSRequest,
    ServeReferenceAudio
)
file_list = sorted([f for f in os.listdir("examples") if f.lower().endswith(('.wav', '.mp3'))])
HEADER_MD = """# üé≠ Fish Speech Dialogue

<div class="container" style="display: flex; width: 100%;">
<div style="flex: 1; padding-right: 20px;">
<h2 style="font-size: 1.5em; margin-bottom: 10px;">–°–∏—Å—Ç–µ–º–∞ –¥–ª—è –æ–∑–≤—É—á–∏–≤–∞–Ω–∏—è –¥–∏–∞–ª–æ–≥–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –≥–æ–ª–æ—Å–∞–º–∏</h2>
<p>‚úèÔ∏è –í—Å—Ç–∞–≤—Ç—å–µ –¥–∏–∞–ª–æ–≥ (—Ä–µ–ø–ª–∏–∫—É), –≥–¥–µ –∫–∞–∂–¥–∞—è —Ä–µ–ø–ª–∏–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∏–º–µ–Ω–∏ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏ –¥–≤–æ–µ—Ç–æ—á–∏—è</p>
<p>üé§ –£–∫–∞–∂–∏—Ç–µ –≥–æ–ª–æ—Å–∞, –Ω–∞–∂–º–∏—Ç–µ –∫–Ω–æ–ø–∫—É –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏</p>
</div>

<div style="flex: 1; padding-left: 20px;">
<h2 style="font-size: 1.5em; margin-bottom: 10px;">–ê–≤—Ç–æ—Ä—ã:</h2>
<p><a href="https://t.me/neuro_art0" style="color: #2196F3; text-decoration: none;">Nerual Dreming</a> ‚Äî –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–æ–≤ - –æ—Å–Ω–æ–≤–∞—Ç–µ–ª—å <a href="https://artgeneration.me" style="color: #2196F3; text-decoration: none;">ArtGeneration.me</a>, —Ç–µ—Ö–Ω–æ–±–ª–æ–≥–µ—Ä –∏ –Ω–µ–π—Ä–æ-–µ–≤–∞–Ω–≥–µ–ª–∏—Å—Ç</p>
<p><a href="https://t.me/FooocusExtend_Support" style="color: #2196F3; text-decoration: none;">Shahmatist^RMDA</a> ‚Äî –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–≤–∫–∏, –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å, –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—ã–π —Ä–µ–ø–∞–∫</p>
<p><a href="https://t.me/neuroport" style="color: #2196F3; text-decoration: none;">üëæ –ù–ï–ô–†–û-–°–û–§–¢</a> ‚Äî –†–µ–ø–∞–∫–∏ –∏ –ø–æ—Ä—Ç–∞—Ç–∏–≤–Ω—ã–µ –≤–µ—Ä—Å–∏–∏ –ø–æ–ª–µ–∑–Ω—ã—Ö –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π</p>
<p><a href="https://t.me/FooocusExtend_Support" style="color: #2196F3; text-decoration: none;">üëæ FooocusExtend</a> ‚Äî –§–æ—Ä–∫ Fooocus c —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º–∏ –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–º–∏ —Ñ—É–Ω–∫—Ü–∏—è–º–∏</p>
</div>
</div>
"""

try:
    import spaces
    GPU_DECORATOR = spaces.GPU
except ImportError:
    def GPU_DECORATOR(func):
        def wrapper(*args, **kwargs):
            with torch.inference_mode():
                return func(*args, **kwargs)
        return wrapper

def normalize_audio_rms(audio, target_db=-20.0):
    current_rms = np.sqrt(np.mean(audio ** 2))
    current_db = 20 * np.log10(current_rms) if current_rms > 0 else -80.0
    gain = 10 ** ((target_db - current_db) / 20)
    return np.clip(audio * gain, -1.0, 1.0)

def build_html_error_message(error):
    return f"""<div style="color: red; font-weight: bold;">{html.escape(str(error))}</div>"""

def get_audio_transcription(audio_path):
    if not audio_path:
        return ""
        
    base_name = os.path.splitext(audio_path)[0]
    
    for ext in ['.txt', '.lab']:
        text_path = base_name + ext
        if os.path.exists(text_path):
            try:
                with open(text_path, "r", encoding="utf-8") as f:
                    return f.read().strip()
            except:
                continue
    return ""

def parse_dialogue(text):
    dialogue_parts = []
    speakers = set()
    current_speaker = None
    current_text = []
    phrases_count = 0
    total_chars = len(text.strip())
    
    if not text or not text.strip():
        return [], 0, 0, 0
        
    for line in text.split('\n'):
        line = line.strip()
        if not line:
            continue
            
        if ':' in line:
            if current_speaker and current_text:
                dialogue_parts.append((current_speaker, ' '.join(current_text)))
                phrases_count += 1
            
            speaker, text = line.split(':', 1)
            current_speaker = speaker.strip()
            current_text = [text.strip()]
            speakers.add(current_speaker)
        elif current_speaker:
            current_text.append(line)
    
    if current_speaker and current_text:
        dialogue_parts.append((current_speaker, ' '.join(current_text)))
        phrases_count += 1
        
    return dialogue_parts, len(speakers), phrases_count, total_chars

def update_dialogue_stats(text):
    _, num_speakers, phrases_count, chars_count = parse_dialogue(text)
    return f"–ì–æ–≤–æ—Ä—è—â–∏—Ö: {num_speakers} | –†–µ–ø–ª–∏–∫: {phrases_count} | –°–∏–º–≤–æ–ª–æ–≤: {chars_count}"

def update_accordion_label(speaker_name, voice_file, index):
    if not speaker_name:
        return f"–ì–æ–≤–æ—Ä—è—â–∏–π {index+1}"
    
    voice_name = os.path.basename(voice_file) if voice_file else "–ù–µ—Ç –≥–æ–ª–æ—Å–∞"
    return f"–ì–æ–≤–æ—Ä—è—â–∏–π {index+1} - {speaker_name} - {voice_name}"

@GPU_DECORATOR
@torch.inference_mode()
def inference(req: ServeTTSRequest, selected_formats):
    refs = req.references
    prompt_tokens = [
        encode_reference(
            decoder_model=decoder_model,
            reference_audio=ref.audio,
            enable_reference_audio=True,
        )
        for ref in refs
    ]
    prompt_texts = [ref.text for ref in refs]

    if req.seed is not None:
        set_seed(req.seed)

    request = dict(
        device=decoder_model.device,
        max_new_tokens=req.max_new_tokens,
        text=req.text,
        top_p=req.top_p,
        repetition_penalty=req.repetition_penalty,
        temperature=req.temperature,
        compile=args.compile,
        iterative_prompt=req.chunk_length > 0,
        chunk_length=req.chunk_length,
        max_length=4096,
        prompt_tokens=prompt_tokens,
        prompt_text=prompt_texts,
    )

    response_queue = queue.Queue()
    llama_queue.put(
        GenerateRequest(
            request=request,
            response_queue=response_queue,
        )
    )

    segments = []

    while True:
        result: WrappedGenerateResponse = response_queue.get()
        if result.status == "error":
            yield None, None, None, None, None
            return

        result: GenerateResponse = result.response
        if result.action == "next":
            break

        with autocast_exclude_mps(
            device_type=decoder_model.device.type, dtype=args.precision
        ):
            fake_audios = decode_vq_tokens(
                decoder_model=decoder_model,
                codes=result.codes,
            )

        fake_audios = fake_audios.float().cpu().numpy()
        segments.append(fake_audios)

    if len(segments) == 0:
        yield None, None, build_html_error_message("–ê—É–¥–∏–æ –Ω–µ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ"), None, None, None
        return

    audio = np.concatenate(segments, axis=0)
    audio_tensor = torch.from_numpy(audio).unsqueeze(0)
    
    audio_paths = {'wav': None, 'mp3': None, 'flac': None}
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    
    for fmt in selected_formats:
        path = os.path.join(outputs_dir, f"output_{timestamp}.{fmt}")
        torchaudio.save(path, audio_tensor, decoder_model.spec_transform.sample_rate)
        audio_paths[fmt] = path

    yield (None, (decoder_model.spec_transform.sample_rate, audio), None, 
            audio_paths['wav'], audio_paths['mp3'], audio_paths['flac'])

    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        gc.collect()

def select_example_audio(audio_file, speaker_name, accordion_index):
    if audio_file:
        audio_path = os.path.join("examples", audio_file)
        transcription = get_audio_transcription(audio_path)
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π –∞–∫–∫–æ—Ä–¥–µ–æ–Ω
        accordion_updates = [gr.update() for _ in range(10)]
        # –û–±–Ω–æ–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–π –∞–∫–∫–æ—Ä–¥–µ–æ–Ω
        accordion_updates[accordion_index] = gr.update(
            label=update_accordion_label(speaker_name, audio_path, accordion_index)
        )
        return [
            audio_path,
            transcription,
            *accordion_updates
        ]
    return [None, "", *[gr.update() for _ in range(10)]]

def on_dialogue_change(text):
    global file_list
    dialogue_parts, num_speakers, phrases_count, chars_count = parse_dialogue(text)
    num_to_show = min(max(num_speakers, 1), 10)
    stats = update_dialogue_stats(text)
    example_audio_files = file_list
    
    updates = []
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –≤—Å–µ—Ö 10 –∞–∫–∫–æ—Ä–¥–µ–æ–Ω–æ–≤
    for i in range(10):
        updates.append(gr.update(visible=(i < num_to_show)))
    
    updates.append(stats)
    updates.append(gr.update(value=num_to_show))
    
    # –û–±–Ω–æ–≤–ª–µ–Ω–∏—è –¥–ª—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –∫–∞–∂–¥–æ–≥–æ —Å–ø–∏–∫–µ—Ä–∞
    for i in range(10):
        if i < len(dialogue_parts):
            # –î–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Å—Ç—Ä–æ–∫ –¥–∏–∞–ª–æ–≥–∞ –±–µ—Ä–µ–º –∏–º—è –ø—Ä—è–º–æ –∏–∑ dialogue_parts
            name = dialogue_parts[i][0]
            updates.extend([
                gr.update(value=name),
                gr.update(),
                gr.update(),
                gr.update(choices=[""] + example_audio_files)
            ])
        else:
            # –î–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Å–ª–æ—Ç–æ–≤ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            name = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {i+1}"
            initial_audio = np.random.choice(example_audio_files) if example_audio_files else ""
            initial_audio_path = os.path.join("examples", initial_audio) if initial_audio else None
            initial_transcript = get_audio_transcription(initial_audio_path) if initial_audio_path else ""
            
            updates.extend([
                gr.update(value=name),
                gr.update(value=initial_audio_path),
                gr.update(value=initial_transcript),
                gr.update(value=initial_audio, choices=[""] + example_audio_files)
            ])
    
    return updates

def update_speaker_visibility(num):
    global file_list
    updates = []
    example_audio_files = file_list
    
    for i in range(10):
        visible = i < num
        
        if visible:
            voice = np.random.choice(example_audio_files) if example_audio_files else ""
            voice_path = os.path.join("examples", voice) if voice else None
            transcript = get_audio_transcription(voice_path) if voice_path else ""
            
            updates.extend([
                gr.update(visible=True),
                gr.update(value=f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {i+1}"),
                gr.update(value=voice_path),
                gr.update(value=transcript),
                gr.update(value=voice, choices=[""] + example_audio_files)
            ])
        else:
            updates.extend([
                gr.update(visible=False),
                gr.update(),
                gr.update(),
                gr.update(),
                gr.update()
            ])
    
    return updates

@GPU_DECORATOR
@torch.inference_mode()
def generate_dialogue_audio(
    text_parts,
    voice_files,
    voice_transcripts,
    max_new_tokens,
    chunk_length,
    top_p,
    repetition_penalty,
    temperature,
    seed,
    selected_formats
):
    audio_parts = []
    
    for speaker, text in text_parts:
        mapped_speaker = speaker
        if speaker not in voice_files:
            try:
                num = int(''.join(filter(str.isdigit, speaker)))
                mapped_index = ((num - 1) % 10)
                visible_speakers = list(voice_files.keys())
                mapped_speaker = visible_speakers[mapped_index]
            except:
                mapped_speaker = list(voice_files.keys())[0]
        
        audio_path = voice_files[mapped_speaker]
        transcription = voice_transcripts.get(mapped_speaker, '') or get_audio_transcription(audio_path)
        
        with open(audio_path, 'rb') as audio_file:
            audio_bytes = audio_file.read()
            
        reference = ServeReferenceAudio(
            audio=audio_bytes,
            text=transcription
        )
        
        req = ServeTTSRequest(
            text=text,
            normalize=False,
            reference_id=None,
            references=[reference],
            max_new_tokens=max_new_tokens,
            chunk_length=chunk_length,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            temperature=temperature,
            seed=int(seed) if seed else None,
            use_memory_cache="never",
        )

        for result in inference(req, selected_formats):
            _, (sample_rate, audio_data), error_msg, wav_path, mp3_path, flac_path = result
            
            if error_msg:
                yield None, None, None
                return
                
            normalized_audio = normalize_audio_rms(audio_data)
            audio_parts.append((sample_rate, normalized_audio))

    if not audio_parts:
        yield None, None, None
        return

    target_sr = audio_parts[0][0]
    combined_audio = []
    
    for sr, audio in audio_parts:
        if sr != target_sr:
            audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)
        combined_audio.append(audio)
    
    final_audio = np.concatenate(combined_audio)
    
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    audio_tensor = torch.from_numpy(final_audio).unsqueeze(0)
    
    wav_path = mp3_path = flac_path = None
    
    if 'wav' in selected_formats:
        wav_path = os.path.join(outputs_dir, f"dialogue_{timestamp}.wav")
        torchaudio.save(wav_path, audio_tensor, target_sr)
        
    if 'mp3' in selected_formats:
        mp3_path = os.path.join(outputs_dir, f"dialogue_{timestamp}.mp3")
        torchaudio.save(mp3_path, audio_tensor, target_sr)
        
    if 'flac' in selected_formats:
        flac_path = os.path.join(outputs_dir, f"dialogue_{timestamp}.flac")
        torchaudio.save(flac_path, audio_tensor, target_sr)

    yield wav_path, mp3_path, flac_path, None

def load_translations(lang):
        file_path = os.path.join('', f'{lang}.json')
        print('---------------------',file_path)
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as file:
                return json.load(file)
        else:
            raise ValueError(f"Translation file for language '{lang}' not found")

current_language = 'ru'
translations = load_translations(current_language)

def set_language(lang):
        global current_language, translations
        translations = load_translations(lang)
        current_language = lang
def _(key):
        return translations.get(key, key)
def change_lang():	
       global current_language
       if current_language=='en':
		      set_language('ru')
       else:
          set_language('en')

with gr.Blocks(theme=gr.themes.Base()) as app:

        gr.Markdown(HEADER_MD)
        lang=gr.Button("Change Language")

        example_audio_files = file_list
        
        app.load(
            None,
            None,
            js="() => {const params = new URLSearchParams(window.location.search);if (!params.has('__theme')) {params.set('__theme', 'dark');window.location.search = params.toString();}}"
        )

        with gr.Row():
            with gr.Column(scale=3):
                initial_text = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –†–µ–±—è—Ç–∞, —É –º–µ–Ω—è –ø—Ä–æ–±–ª–µ–º–∞: –º–æ–π –∫–æ—Ç –ø–æ—Å—Ç–æ—è–Ω–Ω–æ –±—É–¥–∏—Ç –º–µ–Ω—è –≤ 5 —É—Ç—Ä–∞.\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ú–æ–∂–µ—Ç, –æ–Ω —Ö–æ—á–µ—Ç –µ—Å—Ç—å? –ü–æ–ø—Ä–æ–±—É–π –∫–æ—Ä–º–∏—Ç—å –µ–≥–æ –ø–µ—Ä–µ–¥ —Å–Ω–æ–º.\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 3: –ò–ª–∏ –∑–∞–≤–µ–¥–∏ –±—É–¥–∏–ª—å–Ω–∏–∫ –Ω–∞ 4:30 –∏ —Ä–∞–∑–±—É–¥–∏ –µ–≥–æ –ø–µ—Ä–≤—ã–º. –ü—É—Å—Ç—å –∑–Ω–∞–µ—Ç, –∫–∞–∫–æ–≤–æ —ç—Ç–æ!"
                
                dialogue_stats = gr.Textbox(
                    label=i18n('statis_dialog'),
                    value=update_dialogue_stats(initial_text),
                    interactive=False
                    )
                
                dialogue_text = gr.Textbox(
                    label="–¢–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞",
                    value=initial_text,
                    placeholder="–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 1: –ü—Ä–∏–≤–µ—Ç!\n–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å 2: –ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ!",
                    lines=10
                )

                with gr.Row():
                    num_speakers = gr.Slider(
                        label="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –≥–æ–≤–æ—Ä—è—â–∏—Ö",
                        minimum=1,
                        maximum=10,
                        value=3,
                        step=1,
                        interactive=False
                    )

                speaker_boxes = []
                initial_parts, _, _, _ = parse_dialogue(initial_text)
                
                for i in range(10):
                    initial_name = f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å {i+1}"
                    if i < len(initial_parts):
                        initial_name = initial_parts[i][0]
                        
                    initial_audio = np.random.choice(example_audio_files) if example_audio_files else ""
                    initial_audio_path = os.path.join("examples", initial_audio) if initial_audio else None
                    initial_transcript = get_audio_transcription(initial_audio_path) if initial_audio_path else ""
                    
                    with gr.Accordion(
                        label=update_accordion_label(initial_name, initial_audio_path, i),
                        open=False,
                        visible=(i < 3)
                    ) as speaker_accordion:
                        speaker_name = gr.Textbox(
                            label=f"–ò–º—è –≥–æ–≤–æ—Ä—è—â–µ–≥–æ {i+1}",
                            value=initial_name
                        )
                        
                        example_audio = gr.Dropdown(
                            label=f"–ü—Ä–∏–º–µ—Ä –≥–æ–ª–æ—Å–∞ {i+1}",
                            choices=[""] + example_audio_files,
                            value=initial_audio
                        )
                        
                        speaker_voice = gr.Audio(
                            label=f"–ì–æ–ª–æ—Å –≥–æ–≤–æ—Ä—è—â–µ–≥–æ {i+1}",
                            type="filepath",
                            value=initial_audio_path
                        )
                        
                        voice_transcript = gr.Textbox(
                            label="–¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è",
                            lines=3,
                            interactive=True,
                            value=initial_transcript
                        )
                        
                    speaker_boxes.append((speaker_accordion, speaker_name, speaker_voice, voice_transcript, example_audio))

                with gr.Row():
                    with gr.Column():
                        with gr.Accordion(label="–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏", open=False):
                            with gr.Row():
                                chunk_length = gr.Slider(
                                    label="–î–ª–∏–Ω–∞ –∏—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞",
                                    minimum=0,
                                    maximum=300,
                                    value=200,
                                    step=8,
                                )
                                
                                max_new_tokens = gr.Slider(
                                    label="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–∫–µ–Ω–æ–≤",
                                    minimum=512,
                                    maximum=2048,
                                    value=1024,
                                    step=64,
                                )

                            with gr.Row():
                                top_p = gr.Slider(
                                    label="Top-P",
                                    minimum=0.6,
                                    maximum=0.9,
                                    value=0.7,
                                    step=0.01,
                                )

                                repetition_penalty = gr.Slider(
                                    label="–®—Ç—Ä–∞—Ñ –∑–∞ –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–µ",
                                    minimum=1,
                                    maximum=1.5,
                                    value=1.2,
                                    step=0.01,
                                )

                            with gr.Row():
                                temperature = gr.Slider(
                                    label="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞",
                                    minimum=0.6,
                                    maximum=0.9,
                                    value=0.7,
                                    step=0.01,
                                )
                                seed = gr.Number(
                                    label="–°–∏–¥",
                                    value=0,
                                    info="0 –æ–∑–Ω–∞—á–∞–µ—Ç —Å–ª—É—á–∞–π–Ω—É—é –≥–µ–Ω–µ—Ä–∞—Ü–∏—é"
                                )

                            with gr.Row():
                                gr.Markdown("### –§–æ—Ä–º–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
                            with gr.Row():
                                wav_format = gr.Checkbox(label="WAV", value=True)
                                mp3_format = gr.Checkbox(label="MP3", value=False)
                                flac_format = gr.Checkbox(label="FLAC", value=False)

            with gr.Column(scale=3):

                
                with gr.Row(visible=wav_format.value) as wav_panel:
                    audio_wav = gr.Audio(
                        label="WAV",
                        type="filepath",
                        interactive=False,
                        visible=True,
                    )
                with gr.Row(visible=mp3_format.value) as mp3_panel:
                    audio_mp3 = gr.Audio(
                        label="MP3",
                        type="filepath",
                        interactive=False,
                        visible=True,
                    )
                with gr.Row(visible=flac_format.value) as flac_panel:
                    audio_flac = gr.Audio(
                        label="FLAC",
                        type="filepath",
                        interactive=False,
                        visible=True,
                    )

                with gr.Row():
                    generate_button = gr.Button(
                        value="üé≠ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥–∏–∞–ª–æ–≥ (—Ä–µ–ø–ª–∏–∫—É)",
                        variant="primary"
                    )
        mp3_format.change(lambda x: gr.update(visible=x), inputs=mp3_format, outputs=mp3_panel, queue=False)
        wav_format.change(lambda x: gr.update(visible=x), inputs=wav_format, outputs=wav_panel, queue=False)
        flac_format.change(lambda x: gr.update(visible=x), inputs=flac_format, outputs=flac_panel, queue=False)
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞
        dialogue_text.change(
                fn=on_dialogue_change,
                inputs=[dialogue_text],
                outputs=[
                    *[box[0] for box in speaker_boxes],  # 10 –∞–∫–∫–æ—Ä–¥–µ–æ–Ω–æ–≤
                    dialogue_stats,  # —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                    num_speakers,   # –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ø–∏–∫–µ—Ä–æ–≤
                    *[item for box in speaker_boxes for item in box[1:]]  # –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã —Å–ø–∏–∫–µ—Ä–æ–≤
                ]
            )           

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤—ã–±–æ—Ä–∞ –ø—Ä–∏–º–µ—Ä–∞ –∞—É–¥–∏–æ
        for i, (accordion, name, voice, transcript, example) in enumerate(speaker_boxes):
            example.change(
        fn=lambda audio, name, i=i: select_example_audio(audio, name, i),
        inputs=[example, name],
        outputs=[
            voice, 
            transcript,
            *[box[0] for box in speaker_boxes]  # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∞–∫–∫–æ—Ä–¥–µ–æ–Ω–æ–≤
            ]
        )

            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ –ø—Ä–∏ –∏–∑–º–µ–Ω–µ–Ω–∏–∏ –∏–º–µ–Ω–∏
            name.blur(
                fn=lambda n, v, i=i: gr.update(label=update_accordion_label(n, v, i)),
                inputs=[name, voice],
                outputs=[accordion],show_progress=True,queue=False
            )
            name.submit(
                fn=lambda n, v, i=i: gr.update(label=update_accordion_label(n, v, i)),
                inputs=[name, voice],
                outputs=[accordion],show_progress=True,queue=False
            )

        def generate_dialogue(*args):
            dialogue_text_value = args[0]
            num_speakers_value = int(args[1])
            
            speaker_voices = {}
            speaker_transcripts = {}
            
            for i in range(num_speakers_value):
                name = args[2 + i*4]
                voice = args[2 + i*4 + 1]
                transcript = args[2 + i*4 + 2]
                if name and voice:
                    name = name.strip()
                    speaker_voices[name] = voice
                    if transcript:
                        speaker_transcripts[name] = transcript
            
            max_new_tokens_value = args[42]  # 2 + 10*4 = 42 (–Ω–∞—á–∞–ª–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ—Å–ª–µ —Å–ø–∏–∫–µ—Ä–æ–≤)
            chunk_length_value = args[43]
            top_p_value = args[44]
            repetition_penalty_value = args[45]
            temperature_value = args[46]
            seed_value = args[47]
            wav_format_value = args[48]
            mp3_format_value = args[49]
            flac_format_value = args[50]

            selected_formats = []
            if wav_format_value:
                selected_formats.append('wav')
            if mp3_format_value:
                selected_formats.append('mp3')
            if flac_format_value:
                selected_formats.append('flac')
            
            if not selected_formats:
                selected_formats = ['wav']

            dialogue_parts, _, _, _ = parse_dialogue(dialogue_text_value)
            
            if not dialogue_parts:
                return None, None, None, "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞–∑–æ–±—Ä–∞—Ç—å —Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∫–∞–∂–¥–∞—è —Ä–µ–ø–ª–∏–∫–∞ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –∏–º–µ–Ω–∏ –≥–æ–≤–æ—Ä—è—â–µ–≥–æ –∏ –¥–≤–æ–µ—Ç–æ—á–∏—è."
            
            for result in generate_dialogue_audio(
                dialogue_parts,
                speaker_voices,
                speaker_transcripts,
                max_new_tokens_value,
                chunk_length_value,
                top_p_value,
                repetition_penalty_value,
                temperature_value,
                seed_value,
                selected_formats
            ):
                return result

        generate_button.click(
            fn=generate_dialogue,
            inputs=[
                dialogue_text,
                num_speakers,
                *[item for box in speaker_boxes for item in (box[1], box[2], box[3], box[4])],
                max_new_tokens,
                chunk_length,
                top_p,
                repetition_penalty,
                temperature,
                seed,
                wav_format,
                mp3_format,
                flac_format,
            ],
            outputs=[
                audio_wav,
                audio_mp3,
                audio_flac
            ],
            concurrency_limit=1,
        )
        lang.click(change_lang) \
		            .then(lambda: (gr.update()),outputs=dialogue_stats)


def parse_args():
    parser = ArgumentParser()
    parser.add_argument(
        "--llama-checkpoint-path",
        type=Path,
        default="checkpoints/fish-speech-1.5",
    )
    parser.add_argument(
        "--decoder-checkpoint-path",
        type=Path,
        default="checkpoints/fish-speech-1.5/firefly-gan-vq-fsq-8x1024-21hz-generator.pth",
    )
    parser.add_argument("--decoder-config-name", type=str, default="firefly_gan_vq")
    parser.add_argument("--device", type=str, default="cuda")
    parser.add_argument("--half", action="store_true")
    parser.add_argument("--compile", action="store_true", default=True)
    parser.add_argument("--max_gradio_length", type=int, default=0)
    parser.add_argument("--theme", type=str, default="dark")
    parser.add_argument("--share", type=bool, default=False)

    return parser.parse_args()

if __name__ == "__main__":
    os.makedirs("checkpoints", exist_ok=True)
    snapshot_download(repo_id="fishaudio/fish-speech-1.5", local_dir="./checkpoints/fish-speech-1.5")
    print("All checkpoints downloaded")

    args = parse_args()
    args.precision = torch.half if args.half else torch.bfloat16

    logger.info("Loading Llama model...")
    llama_queue = launch_thread_safe_queue(
        checkpoint_path=args.llama_checkpoint_path,
        device=args.device,
        precision=args.precision,
        compile=args.compile,
    )
    logger.info("Llama model loaded, loading VQ-GAN model...")

    decoder_model = load_decoder_model(
        config_name=args.decoder_config_name,
        checkpoint_path=args.decoder_checkpoint_path,
        device=args.device,
    )



    logger.info("Warming up done, launching the web UI...")

    
    app.launch(show_error=True, show_api=True, inbrowser=True, share=args.share)
